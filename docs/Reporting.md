# Report and Visualization

The report is a single self‑contained HTML page generated by `build_bench_report.py` and automatically written as `index.html` in the run folder.

Features
- Sortable, filterable table of all models.
- Show/hide columns via toggles (timestamp, settings, totals, CPU/ANE W hidden by default).
- Links per model: `HTML` (rendered site), `Text` (raw output), `JSON` (metrics).
- Collapsible section with prompt text, machine info (CPU/GPU/RAM), generation summary.
- Charts (inline canvas):
  - Scatter: tokens/sec vs GPU avg W.
  - Bars: generation time per model (top 20).

Columns (selected)
- Model: model id used for generation (REST API id).
- Load s, Gen s: load time and generation time in seconds.
- Tok/s: tokens per second (server stats if available, else derived).
- Model size: parsed parameter size in billions (e.g., `21.0B`).
- Quantization: reported quantization.
- LM RSS Δ load / Δ gen: deltas for LM Studio RSS after load and after generation.
- Artifacts: links to outputs (HTML, Text, JSON).

Data sources
- Each row corresponds to one `<model>.json` produced by the benchmark.
- Tokens/sec: `rest_stats` if present; fallback uses `completion_tokens / generation_time_seconds`.
- Model size: extracted from `model_info`/`runtime` or inferred from id/strings where possible.

Standalone usage
- `python3 build_bench_report.py reports/lmstudio-bench-YYYYMMDD-HHMMSS --out report.html`
- Optional: `--prompt-file prompt.txt` if prompt isn’t present in JSON.

Customization ideas
- Add more charts (e.g., load time bars, energy estimates from power over time if sampling window is available).
- Add CSV export of the table.
- Combine multiple runs in a single report with a “Run” column.

