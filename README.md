# LLM Benchmark Suite

Ein umfassendes Benchmarking-Tool fÃ¼r Large Language Models (LLMs), das sowohl lokale Modelle (via LM Studio) als auch API-basierte Modelle (via OpenRouter) testet und vergleicht.

## ğŸ“‹ Ãœbersicht

Dieses Projekt ermÃ¶glicht es, LLMs systematisch zu benchmarken und detaillierte Performance-Metriken zu erfassen:

- **Lokale Modelle (LM Studio)**: Misst Ladezeit, Generierungszeit, Tokens/Sekunde, GPU/CPU-Leistung und Speicherverbrauch
- **API-Modelle (OpenRouter)**: Testet verschiedene Cloud-Modelle und erfasst Geschwindigkeit, Token-Nutzung und Kosten
- **Interaktive Reports**: Generiert HTML-Berichte mit Screenshots, sortierbaren Tabellen und Visualisierungen

## ğŸ¯ Features

- âš¡ **Performance-Metriken**: Ladezeit, Generierungszeit, Tokens/Sekunde
- ğŸ’» **Hardware-Monitoring**: CPU/GPU-Leistung, Speicherverbrauch (macOS)
- ğŸ’° **Kosten-Tracking**: API-Nutzungskosten fÃ¼r OpenRouter-Modelle
- ğŸ“Š **Interaktive Reports**: Sortierbare Tabellen, Screenshots beim Hover, Diagramme
- ğŸ”„ **Parallele AusfÃ¼hrung**: OpenRouter-Benchmarks laufen parallel fÃ¼r schnellere Ergebnisse
- ğŸ“ **Detaillierte Logs**: JSON-Outputs mit allen Metriken und generierten HTML-Artefakten

## ğŸ› ï¸ Installation

### Voraussetzungen

- Python 3.9 oder hÃ¶her
- macOS (fÃ¼r Power-Metriken mit `powermetrics`)

### Schritt 1: Repository klonen

```bash
git clone <repository-url>
cd lmTestAuto
```

### Schritt 2: Python-AbhÃ¤ngigkeiten installieren

```bash
python3 -m pip install -r requirements.txt
```

### Schritt 3: Optional - Playwright fÃ¼r Screenshots installieren

FÃ¼r die Screenshot-Funktion in den Reports:

```bash
pip install playwright
playwright install chromium
```

### FÃ¼r LM Studio Benchmarks

1. **LM Studio installieren**: Version 0.3.6 oder hÃ¶her von [lmstudio.ai](https://lmstudio.ai)
2. **CLI aktivieren**: In LM Studio Settings â†’ Developer â†’ Enable CLI
3. **Local Server aktivieren**: In LM Studio â†’ Local Server starten (EULA akzeptieren)
4. **Modelle herunterladen**: GewÃ¼nschte Modelle in LM Studio herunterladen

### FÃ¼r OpenRouter Benchmarks

1. **OpenRouter API Key erstellen**: Auf [openrouter.ai](https://openrouter.ai) registrieren und API-Key erstellen
2. **API Key als Umgebungsvariable setzen**:
   ```bash
   export OPENROUTER_API_KEY="your-api-key-here"
   ```

## ğŸš€ Verwendung

### 1. LM Studio Benchmarks (Lokale Modelle)

Benchmarkt alle lokal installierten Modelle:

```bash
# Mit sudo fÃ¼r Power-Metriken (empfohlen auf macOS)
sudo -E python3 bench_lmstudio_models.py

# Ohne sudo (keine Power-Metriken)
python3 bench_lmstudio_models.py
```

**Was wird gemacht:**
- Listet alle verfÃ¼gbaren lokalen Modelle auf
- LÃ¤dt jedes Modell und misst die Ladezeit
- Generiert eine Test-Website mit jedem Modell
- Erfasst Performance-Metriken (Tokens/s, GPU/CPU-Leistung, RAM)
- Speichert HTML-Output und JSON-Metriken

**Output:** `reports/lmstudio-bench-YYYYMMDD-HHMMSS/`

### 2. OpenRouter Benchmarks (API-Modelle)

#### Schritt 1: Modell-Liste erstellen

Erstelle eine Textdatei mit Modellnamen (ein Modell pro Zeile):

```bash
# openrouter_models.txt
openai/gpt-4-turbo
anthropic/claude-3-opus
google/gemini-pro
```

Oder verwende eine vorgefertigte Liste:
- `openrouter_models.txt` - Kleine Auswahl fÃ¼r schnelle Tests
- `all_relevant_openrouter_models.txt` - Umfassende Liste vieler Modelle

#### Schritt 2: Benchmark ausfÃ¼hren

```bash
# Standard (4 parallele Requests)
python3 bench_openrouter_models.py --models_file openrouter_models.txt

# Mit mehr ParallelitÃ¤t (8 gleichzeitige Requests)
python3 bench_openrouter_models.py --models_file openrouter_models.txt --concurrency 8

# GroÃŸe Liste testen
python3 bench_openrouter_models.py --models_file all_relevant_openrouter_models.txt --concurrency 6
```

**Was wird gemacht:**
- Sendet den gleichen Prompt an jedes Modell
- Misst Generierungszeit und Tokens/Sekunde
- Erfasst Token-Nutzung und Kosten
- Extrahiert HTML aus der Antwort
- Speichert JSON-Metriken und HTML-Output

**Output:** `docs/openrouter-bench-XXXXXXXX/`

### 3. Reports generieren

#### LM Studio Report

```bash
# Automatischer Report (wird beim Benchmark erstellt)
# Manuell neu generieren mit Screenshots:
python3 build_bench_report.py reports/lmstudio-bench-YYYYMMDD-HHMMSS

# Ohne Screenshots (schneller):
python3 build_bench_report.py reports/lmstudio-bench-YYYYMMDD-HHMMSS --no-screenshots

# Custom Output-Pfad:
python3 build_bench_report.py reports/lmstudio-bench-YYYYMMDD-HHMMSS --out my_report.html
```

#### OpenRouter Report

```bash
# Report aktualisieren (mit Screenshots):
python3 openrouter_report.py docs/openrouter-bench-XXXXXXXX

# Ohne Screenshots:
python3 openrouter_report.py docs/openrouter-bench-XXXXXXXX --no-screenshots
```

**Report-Features:**
- âœ… **Sortierbare Spalten**: Klick auf SpaltenÃ¼berschriften zum Sortieren
- ğŸ–¼ï¸ **Screenshot-Preview**: Hover Ã¼ber Zeile zeigt Screenshot der generierten Seite
- ğŸ“Š **Diagramme**: Visualisierungen fÃ¼r Tokens/s vs. GPU-Leistung
- ğŸ” **Filter**: Suchfeld fÃ¼r Modellnamen
- ğŸ‘ï¸ **Spalten ein/ausblenden**: Toggles fÃ¼r jede Spalte
- ğŸ“… **Timestamp**: Zeigt wann jeder Test durchgefÃ¼hrt wurde

## ğŸ“ Projektstruktur

```
lmTestAuto/
â”œâ”€â”€ bench_lmstudio_models.py      # LM Studio Benchmark-Skript
â”œâ”€â”€ bench_openrouter_models.py    # OpenRouter Benchmark-Skript
â”œâ”€â”€ build_bench_report.py         # Report-Generator fÃ¼r LM Studio
â”œâ”€â”€ openrouter_report.py          # Report-Generator fÃ¼r OpenRouter
â”œâ”€â”€ requirements.txt              # Python-AbhÃ¤ngigkeiten
â”œâ”€â”€ prompt_kanban.md             # Beispiel-Prompt (Kanban Board)
â”œâ”€â”€ prompt_skillManagement.md    # Beispiel-Prompt (Skill Management)
â”œâ”€â”€ openrouter_models.txt        # Beispiel Modell-Liste
â”œâ”€â”€ reports/                      # LM Studio Benchmark-Ergebnisse
â”‚   â””â”€â”€ lmstudio-bench-YYYYMMDD-HHMMSS/
â”‚       â”œâ”€â”€ index.html           # Generierter Report
â”‚       â”œâ”€â”€ MODEL_NAME.json      # Metriken pro Modell
â”‚       â”œâ”€â”€ MODEL_NAME.html      # Generierte Website
â”‚       â”œâ”€â”€ MODEL_NAME_screenshot.png  # Screenshot
â”‚       â””â”€â”€ MODEL_NAME_powermetrics.log
â””â”€â”€ docs/                        # OpenRouter Benchmark-Ergebnisse
    â””â”€â”€ openrouter-bench-XXXXXXXX/
        â”œâ”€â”€ index.html           # Generierter Report
        â”œâ”€â”€ MODEL_NAME.json      # Metriken pro Modell
        â”œâ”€â”€ MODEL_NAME.html      # Generierte Website
        â””â”€â”€ MODEL_NAME_screenshot.png
```

## âš™ï¸ Konfiguration

### Prompt anpassen

Beide Skripte verwenden einen vordefinierten Prompt. Du kannst ihn direkt in den Skripten Ã¤ndern:

```python
# In bench_lmstudio_models.py oder bench_openrouter_models.py
PROMPT = """
Dein eigener Prompt hier...
"""
```

Oder verwende eine externe Datei:

```bash
# prompt.txt erstellen mit deinem Prompt
python3 bench_openrouter_models.py --models_file models.txt --prompt "$(cat prompt.txt)"
```

### Parameter anpassen

In den Skripten am Anfang:

```python
# Temperature (KreativitÃ¤t): 0.0 - 2.0
TEMP = 0.6

# Top P (Nucleus Sampling): 0.0 - 1.0
TOP_P = 0.95

# Max Tokens (AntwortlÃ¤nge): -1 fÃ¼r unbegrenzt
MAX_TOKENS = -1

# GPU Setting (nur LM Studio): "max", "off"
GPU_SETTING = "max"
```

## ğŸ“Š Erfasste Metriken

### LM Studio (Lokale Modelle)

| Metrik | Beschreibung |
|--------|--------------|
| **load_time_seconds** | Zeit zum Laden des Modells |
| **generation_time_seconds** | Zeit fÃ¼r die Antwort-Generierung |
| **tokens_per_second** | Generierungsgeschwindigkeit |
| **prompt_tokens** | Anzahl Input-Tokens |
| **completion_tokens** | Anzahl generierte Tokens |
| **cpu_w_avg/max** | CPU-Leistungsaufnahme (Watt) |
| **gpu_w_avg/max/min** | GPU-Leistungsaufnahme (Watt) |
| **ane_w_avg** | Apple Neural Engine Leistung |
| **mem_after_load_lms** | RAM-Nutzung nach Laden |
| **mem_after_gen_lms** | RAM-Nutzung nach Generierung |
| **model_size** | ModellgrÃ¶ÃŸe (Parameter) |
| **quantization** | Quantisierung (4bit, 8bit, etc.) |

### OpenRouter (API-Modelle)

| Metrik | Beschreibung |
|--------|--------------|
| **generation_time_seconds** | API-Antwortzeit |
| **tokens_per_second** | Generierungsgeschwindigkeit |
| **prompt_tokens** | Anzahl Input-Tokens |
| **completion_tokens** | Anzahl generierte Tokens |
| **cost** | Kosten in USD |
| **timestamp** | Zeitpunkt des Tests |

## ğŸ”§ Troubleshooting

### LM Studio

**Problem:** `lms` Kommando nicht gefunden
- **LÃ¶sung**: In LM Studio Settings â†’ Developer â†’ Enable CLI aktivieren

**Problem:** Server startet nicht
- **LÃ¶sung**: LM Studio Ã¶ffnen und Local Server manuell starten
- PrÃ¼fen ob Port 1234 frei ist: `lsof -i :1234`

**Problem:** Keine Power-Metriken
- **LÃ¶sung**: Skript mit `sudo -E` ausfÃ¼hren
- Auf macOS: `powermetrics` sollte verfÃ¼gbar sein (`which powermetrics`)

**Problem:** Keine Modelle gefunden
- **LÃ¶sung**: Modelle in LM Studio herunterladen
- PrÃ¼fen: `lms ls --llm` sollte Modelle auflisten

### OpenRouter

**Problem:** API-Fehler "Unauthorized"
- **LÃ¶sung**: `OPENROUTER_API_KEY` Environment Variable korrekt setzen
- PrÃ¼fen: `echo $OPENROUTER_API_KEY`

**Problem:** "Rate limit exceeded"
- **LÃ¶sung**: `--concurrency` reduzieren (z.B. auf 2 oder 3)
- Pausen zwischen Requests einbauen

**Problem:** Timeouts bei groÃŸen Modellen
- **LÃ¶sung**: Timeout in `bench_openrouter_models.py` erhÃ¶hen:
  ```python
  TIMEOUT = 600  # 10 Minuten statt 5
  ```

### Reports

**Problem:** Screenshots werden nicht erstellt
- **LÃ¶sung**: Playwright installieren:
  ```bash
  pip install playwright
  playwright install chromium
  ```

**Problem:** Report zeigt keine Daten
- **LÃ¶sung**: PrÃ¼fen ob JSON-Dateien im Verzeichnis vorhanden sind
- Pfad zum Report-Verzeichnis korrekt angegeben?

## ğŸ“š Weitere Dokumentation

- `docs/Overview.md` - Detaillierte ProjektÃ¼bersicht
- `docs/Benchmarking.md` - Benchmark-Ablauf und Konfiguration
- `docs/Reporting.md` - Report-Struktur und Features
- `CHANGELOG.md` - Versionshistorie

## ğŸ¤ Beitragen

Contributions sind willkommen! Bitte erstelle einen Pull Request oder Ã¶ffne ein Issue fÃ¼r VerbesserungsvorschlÃ¤ge.

## ğŸ“ Lizenz

[Lizenz hier einfÃ¼gen]

## ğŸ™ Credits

Entwickelt fÃ¼r systematische LLM-Evaluierung und Performance-Vergleiche.

---

**Hinweis**: FÃ¼r genaue Power-Metriken auf macOS werden Admin-Rechte benÃ¶tigt (`sudo`). Das Skript funktioniert auch ohne, erfasst dann aber keine Power-Daten.
